2017-01-19 Production team testing synch to HH
Thomas M is taking care of this. After these tests FTP service is ready to fully migrate.

2017-01-18 FTP FUSE fs access tested
Web team report tests are successful

2016-12-20 Follow up on volume synch
Main question is how rapidly a large directory structure can be made public. Lets take /nfs/ensemblftp/PUBLIC/pub/release-87 as an example - this is 1.3Tb and we'd typically assemble it over a number of days as finished data is available to us. We then want to make the entire directory tree public in one go. What strategies do they recommend for this, and how "instantaneous" would they be?

From sysinf:
We can snapmirror the data every 30 mins to try to keep the data up to date as possible.    How long a particular snapmirror transfer takes will depend on how much data you have changed.  What I can tell you is that the snapmirror protocol is block based and extremely efficient so in terms of comparison to other tools,  it is going to be one of the 
fastest.

It sounds like we at least need to ensure that you are able to create a snapshot so you have a baseline for a release.  How many snapshots (i.e. releases) do you think you will need to have online?   If you need lots of them,  it can be very expensive in terms of space as you may be keeping multiple copies of data.   If you are keeping a limited number of previous versions,   it will be much more manageable.

Looking at your transfers now,  they run at 15 mins past every hour.   A quick look tells me they taking no longer than 30 secs but perhaps you are not making any changes to that data in the past week?

From Dan:
So, the release cycle we have is to build up a set of release files of ~2tb over about 2-3 weeks. This should not be available to the public via the FTP site until we are ready to release the website, at which point it needs to be published more or less instantaneously.

Outside of this time, changes will be very minimal until the next cycle starts - so for the moment we're not making any changes.

2016-12-15 FTP volume synch clarification from sysinf
From Dan:
First, this is the volume in question:
Filesystem                         1K-blocks        Used   Available Use% Mounted on
vnas-ensemblftp:/ivol_ensemblftp 71403831296 49116182720 22287648576 69% /nfs/ensemblftp

So, our planned usage is to copy a large-ish amount of data (1-2Tb) onto this file system once every 2-3 months, representing a release.

Whilst the copy of new data onto the volume will be iterative over a number of days, we want to make all this data publicly accessible as quickly as possible at release time, preferably instantaneously, so users aren't seeing partial loads on the site.

For instance, we want to populate a new release directory e.g. /nfs/ensemblftp/PUBLIC/pub/release-88, but only have it accessible once the directories below are fully populated e.g. by changing permission. However, it was not clear what the lag time would be between making these changes in HX and having them live in HH. If you can provide more information, that'd be great.

Marc said he'd look into strategies for doing this but we agreed that it would make sense initially to have the mirroring to london triggered by our group rather than being automatic, and that Marc would provide a script for us to run as ensemblftp when we're ready to go live.

From sysinf:
The volume is currently snapmirrored at 15 minutes past the hour every hour. This means that the data is synced to an offsite volume. This can be increased to a maximum of every 30 minutes.

The advantage of a regular snapmirror is that it means that the snapmirror target is kept more regularly up to date. If the target is not updated regularly,  it also means that when an update does occur, it takes much longer to transfer the data and will be more unpredictable as to when it will complete. The regular snapmirror, also means that we have a regular backup of this data to an offsite location. It means that if we lose the source,  we can quickly switch to the target volume with little loss of data.

In terms of setting up an adhoc release process for the team, this would need to be discussed with Pete. In the first instance we try to use recognised released mechanisms that the sysinf team have setup to avoid having many individually created scripts that can be difficult to 
maintain and document. Or,  we rely on out of the box Vendor applications that can easily be setup and managed such as in this case, the NetApp snapmirror application which is an excellent replication tool.

2016-12-09 FUSE to EBI is fine

2016-12-01 FTP/HTTP protocols work as expected
Need to test rsync

2016-11-25 Sysinf has implemented new layout
Applied the changes that will allow the http ftp and rsync to serve from PUBLIC, and http to hide PRIVATE but provide it if requested and with support for .htaccess.

2016-11-18 Agreement on FTP layout with sysinf
- <ROOT>/pub folder will be renamed to <ROOT>/public.
- inside public a pub folder will exist to keep all actual functionalities.
- a new private folder will be created at Root level. <ROOT>/private
- All services (ftp.http,rsync) will have the root level on the <ROOT>/public
folder.
- The HTTP service will have an extra 'binding' from <ROOT>/private to
<ROOT>/public/private , this way the folder will be seen from the service root
level and your team will be able to manage the ACL from a .htaccess file.

- Regarding backups, daily snapshots are in place for as long as 7 days. There
is more information on this topic on the following link
https://docs.google.com/document/d/14y1CK8e6U7ZOnBNoSfXkELXmfdI5jLSDe7WNHuxp4s4/edit#

- Regarding your concern on how long does it take to move from private to
public, it is expected to be almost immediate since they are on the same
storage .

2016-11-09 HTTP Rsync available in HH instances
sysinf will look into provide ftp user restricted access to a private area first, then http with .htaccess based site access.

2016-10-31 Raised ticket to sysinf 
HTTP/Rsync access, private mount point

2016-10-31 Data transfer to HX mount point 90%

2016-10-25 Data transfer to HX mount point 66%

2016-10-24 Email to Asier asking FTP set up

2016-10-11 Data transfer to HX mount point 15%

2016-10-10 Data transfer to HX mount point 10%
ETA end of month.

2016-10-08 Initiated data transfer to FTP mount point at HX
A total of 42T.

2016-10-07 Completed FTP transfer to EBI
Available at /nfs/ensemblarch/ftp

2016-09-16 Confirmation from EBI systems
Data can put at /nfs/ensemblftp, which will then sync to HH every hour.
Granted privilege to become ensemblftp, need to force shell, i.e. $ become ensemblftp /bin/bash

2016-09-14 FTP transfer update
Looking at two more weeks or so to go.

2016-09-13 FTP servers ready in London
Need to start testing external FTP/HTTP access, requested London FTP set up status to Asier.

Asier confirmed they're ready: [ftp,http,rsync]://ftp.ensemblorg.ebi.ac.uk
He needs to talk to storage team but he thinks there was an active copy from HX to HH for publishing data. Will let us know.

2016-09-12 FTP copying status update
Still going on, expected to complete next week.

2016-09-05 FTP copying status update
Only 8.7Tb so far. Not in a rush though.

2016-09-02 Migration update
Data is currently copying to EBI, but exactly how to deploy here is not yet fully defined. We therefore need to release the 86 data on the current Sanger hardware 

2016-09-01 Started copying FTP data
Copying from mount on ensweb-1-19 to /nfs/ensemblarch/ftp started. 39Tb - will take at least a week I reckon

Note I'm only copying ensembl/pub, ignoring ensembl/bin etc

I think we should see if we can archive some of this - what is assembly/mouse/mgsc_assembly_1 for example ?

2016-08-24 Shadowland FTP endpoint
Steve to ask shadowland endpoint for FTP next week.

2016-08-22 Warehouse for FTP ready
/nfs/ensemblarch/ftp ready.

2016-08-19 Shadowland FTP endpoint & C.
Steve asked warehouse subdir /nfs/ensemblarch/ftp to store FTP data.

Requisition machines/internal beta testing/data move: haven't yet been assigned (one of us or web depending on time), need to set up asana/gaant for that

Missing "redirect domain from Sanger to EBI" from plan

Well we have two modes

1. Partial redirection. Ask sanger to redirect X.ensembl.org to EBI load balancers.
2. Total redirection. All services (or most) need to be here at EBI with rules on the EBI LBs to either point to the right internal resource (we need a service map) or back to Sanger. Sanger will switch their DNS expiry to 5 minutes a week or so before the move. When the switch happens EBI sets their expiry to a reasonable amount of time. Overall effect is within 5 minutes ensembl.org will move from Sanger to EBI.

2016-08-18 FTP and mirrors
Mirrors are loaded with a fraction of the data required from FTP. Service will be up, but with slightly reduced functionality.

2016-08-17 Shutdown and mirrors
Shutdown: affects mirrors by FTP-provided flat-file loading (e.g. BAM) facility from FUSE mounts, might be bypassed, how?

2016-08-04 Internal beta FTP testing
Internal beta testing is now enabled for FTP, HTTP and RSYNC.

- FTP: ftp://hh-ens-ftp-001.ebi.ac.uk
- HTTP: http://hh-ens-http-001.ebi.ac.uk
- RSYNC: rsync://hh-ens-rsync-001/

Andy wants rsync access to avoid ascp or FTP transfer

2016-08-03 FTP storage and servers
ensemblftp is now on the dedicated link to Sanger. once logged in you see two subfolders,

ensemblorg -> on the EBI cluster /nfs/ftp/ensemblorg
ensemblftp -> on the EBI cluster /nfs/ensemblftp  

Can start with transfers right now, can use aspera but need to perform a number of re-syncs of Sanger based FTP to EBI based FTP before we are sure the service is ready to go live. ascp can provide resuming and do some basic comparison of files (or full md5 if you need to) and move it across. Asier installing rsync or we go for aspera?

Setting up the FTP servers in London.

2016-08-02 Shutdown and mirrors
Need an FTP site for the mirrors (load flat files from FTP into live tools), or could have a reduced level of service for these (shutdown).

2016-07-27 FTP storage
HX FTP serving area is now mounted at /nfs/ensemblftp and available on the EBI cluster. there was another volume to transfer data from/to Sanger (/nfs/ftp/ensemblorg/pub/).

2016-07-15 Proposed FTP layout
### Proposed FTP layout 

75TB usable ftp space in London and Hinxton. One of them will be a read-only copy (ideally London). Can ingest data only to one (the read-write). ftp service will initially live in one location. They will be mirrored. There's no failover at the moment, semi-manual process.

Data ingest in Hinxton. Serving from London, ie. 2 load balanced ftp servers in London. Mirror every hour. No failover, no need to go too complicated here since this is a failover for files we need from FTP to run the website not for external users (?!) ftp.ensembl.org to point to LDC and ftp-ensembl.ebi.ac.uk to point to a HX FTP/HTTP server and manage the switchover manually.

2016-07-07 FTP migration update
FTP will not migrate until later on in the year. We found out the current order lacked FTP space (an oversight). This is pending a meeting with Pete J @ EBI

2016-06-29 HW provision does not include FTP storage
Andy C informed us the HW provision does not include the requested storage for FTP (~100TB), which will be covered when the Sanger grant starts in October.
Pb is that we would like to migrate the existing FTP content before that date, for which we would need ~35TB. Andy told us we would need to speak with Pete J to see if it's possible to find a temporary solution.

2016-04-05 Problem with aspera copy from Sanger solved
Problem with aspera copy from Sanger solved (wrong permission on ftp user folder). Successfully transferred release 82/83 FTP data. Data in subdirectories with symlinks has not been copied, need to take that into account when setting it up. Asked systems to set up FTP service, need some time to do that.

2016-02-23 Successfully tested FTP transfer
Successfully tested FTP transfer, but not feasible for transferring data for entire releases. Tested aspera copy, issue with SSH connection (session initiation failed).
Issue with file permission when copying data directly to mount point (not owned by ftp user).

2016-01-25 Mount point enabled on EBI cluster
Mount point enabled on EBI cluster (/nfs/ftp/ensemblorg). Can start transferring data by submitting LSF jobs.

2016-01-12 Volume for data transfer test
5TB volume to test data transfer and backup volume created.

2015-09-17 Pilot study into the move starts
Pilot study into the move starts. Sysinf to look into allocating 50TB on their NetApp cluster and sort out the new FTP service.
