24/08/2016
talk with Dan S about production migration plans/shutdown (anytime after 10:30am)

organise meeting for warehouse

23/08/2016
RT migration catch up

user/hive/tools DB should have been transferred (pm), ask Steve

ask Andy if he agress on production suicidal priority list (include staging1/2 in web live slot?)


22/08/2016
==========
X contact Dan S about production/shutdown
- shutdown from the perspective of production
  - 86 work will be finished, genebuilders might be running some pipeline, they're aware they have to use non-faulty HW
  - we're back to staging 1/2, be sure to go back to non-suicidal before shutdown
	(1) We'll announce that we're shutting down processes on staging-1/2 on Thursdy afternoon and all pipelines need to finish by then
	(2) We'll copy everything from staging-1/2 to ens-staging3 on Thursday afternoon
	(3) We'll cross our fingers that staging-1/2 come back to life, but need confirmation from systems that they'll be able to source a suitable replacement in a day or two in the event it doesn't come back.
  - Micheal raised a point: storage is not affected, but how to recover the mount points from faulty machines in case they don't survive? Should we take note of these?
    That's not a problem, as the disk with data contains the OS as well
  - lustre warehouse not affected , pipelines can continue safely
- we have
  - 100TB of clustered file system (lustre, total usage ATMO is 43TB) attached to HX farm. Can use 'our share' of the 5PB Lustre just published
  - 300TB of NFS/medium storage areas (longer term warehousing for lustre's processes backups, currently at 200TB including the flat-file stores)
  - NFS area attached to HX farm (/nfs/software/ensembl ensw rw access only) for software installation (RH7)
  - HW request form sent to request production HW, available after shutdown
- we need
  - strategy for moving warehouse
  - DB server move, cleaning first?, version strategy? when, who, how?
  - communicate list of servers which will require attention at reboot (Helen ? from ISG deals with e!), ST will probably do that for web including production machines
- timeline for migration, what he needs for that to happen, should be in charge, need to set a date and tell the other teams

X migration meeting

X asana/gaant of FTP start

X steve to receive production HW priority list for shutdown

X allocate extra storage for archive NFS server

19/08/2016
X update HW procurement docs based on confluence

X Sanger migration meeting minutes

X send Steve updates on yesterday's discussion to complete

X ask Andy plan about FTP, what happened to CVS (there's a page on confluence), projects.ensembl.org migration to Jekyll, ask to contribute

work on data review slides, email Paul F 12/08 (ensembl campus) about shutdown

18/08/2016
X prob Dan about production meeting, seems he responds to email. Meet on Monday via Slack

X meet Steve in the morning (to become regular, eventually), shutdown recap et al.
- ensembldb transfer not going at full speed, DMZ 1G link. ETA: middle next week, martdb missing (asked MB 22TB data)
- FTP ready for transfer/testing: who/when/how (rsync?) do the transfer, test FTP/HTTP/rsync servers, who/when? Andy's in charge for that
- warehouse: transfer follows after ensembldb, when/who/where? what need to be done, cleaning?
- NFS data move: who/when/how?
- projects.ensembl.org available as tarball, what to do?
- anything else?
- shutdown from web perspective: 
  - architecture coping shutdown: 
    - shadowland is for webservers (incl archives)/web LSF to connect to user/hive dbs at EBI, anything else (apart from data transfer), ready?
    - ensembl user/hive dbs (VPN with AWS OK, shadowland/internal network links? for archives/web servers). why was it decided to transfer them first before shutdown, because of suicidal blades? temporary disaster recovery architecture with HH failover
    - web servers: some are on suicidal blades, what happens if something goes wrong? web app components transferred to non suicidal then recreated? can we wait the overhead, has been tested?
    - archives: remain at Sanger, access user/hive at EBI via shadowland?
    - FTP won't be moved before fall: steps?
    - public MySQL: where are they, are they safe? deadline for migration, what need to be done for that to happen? old version of Mysql running there, which ones are we going to updgrade to 5.6, and how. if we can do for it, we have a procedure for the production DBs
    - mirrors (AWS?): affected by FTP-provided flat-file loading (e.g. BAM) facility from FUSE mounts, can it be bypassed/done elsewhere wo FTP (strategy?) or do we accept service degradation
    mirrors have a copy of some of the required data (AceDB/VCF files), missing BAM & C. files from regulation/genebuild. Service will be available, graceful degration
  - disaster recovery architecture long-term solutions

X migration meeting with Sanger: catch up on how the data and VM migration is going and whether there are any issues
- archives: data moving (ensembldb), 81-85 on hold at the moment (DBA vacation) Pbs. with 67 instance, Luke to further investigate and update on Friday, Dan suggests rsync directories out of running instance to blank one. Asked archive immediately before/after that one, no reply. NFS data move? Mark was to propose solution (12/07), can possibly use shadowland
  54/67 will need to migrate together since they share a database
Luke's on holiday, Paul to contact him about our issues
- FTP: 75TB storage available, internal beta testing enabled for FTP/HTTP/RSYNC, testing/data move by rsync commence soon
- public MySQL: transferring ensembldb, 31/54TB done, ETA: middle next week, migration timeline: two services/2 servers, request 2 VMs each in HX/HH (fail over), upgrade to 5.6 at EBI, keep MySQL 4. Aiming at e!86 (Sep 20)
  martdb not yet transferred (contacted Martin Burton), need to copy the same way (22TB)
- production:
  - HW request by form for production machines, likely to be available just after shutdown, need to know precise dates when production moves (will talk soon to coordinator)
  - pipeline testing going on: core (~100%), variation (70%), regulation (0%), genebuild (10-25%?), compara ?
  - long-term software environment almost ready
  - farm/warehouse filesystems ready and accepting/receiving data, warehouse data transfer follows ensembldb
- web mainly involved in shutdown: should be almost safe (AWS VPN OK, Sanger VPN OK, user/hive at EBI ready by Monday), web to send a list of webservers which will require attention during shutdown (Helen Brimber)
frontend (Sawston building), www.ensembl.org presents a page with links to the mirrors
once machines are ready, can start switching DNS

X logs/milestones by WP: start

17/08/2016
X contact Steve, ask catch up about shutdown 
X enquire Andy about: 
- HW request form (done last week, will likely have after shutdown)
- verify we have 300TB of NFS/medium storage for warehouse (yes, Steve is transferring ensembldb to it)
- FTP: rsync? must be at EBI for shutdown? (mirrors), internal beta testing, who does what and when? data move? need a decision
- meet at some point about RH6/7

16/08/2016
X finish set up new archive server (load data, set networking)
X expand mart server storage (13TB)
X inform Andy about genBlast (redirect to Fergal)
enquire Andy about HW request form and FTP/CVS (meeting at some point about RH6/7)
X catch up with FTP migration details (e! next purchase thread, mainly)
X probe Luke (give reasonable timeline).

15/08/2016
X migration meeting
X set up new main archive server
X Andy Y to be informed about genBlast, got explanation from Thibaut
